# 在当前目录下创建新的scrapy项目
$scrapy startproject wikiSpider
    #wikiSpider文件夹的目录结构
    #--wikiSpider
        #--__init.py__
        #--items.py
        #--pipelines.py
        #--settings.py
        #--spiders
        #--__init.py__
#为了创建一个爬虫，需要在wikiSpider/wikiSpider/spiders/ 文件里增加一个articleSpider.py文件
#在items.py文件中，需要定义一个Article类

#items.py 文件如下，Scrapy自动生成的注释内容
#_*_ coding: utf-8 _*_
#Define here the models for your scraped items
#
#See documentation in;
#http://doc.scrapy.org/en/latest/topics/items.html

form scrapy import Item, Field

class Article(Item):
    #define the fields for your item here like:
    #name = scrapy.Field()
    title = Field()

# 在新建的articleSpider.py文件里
from scrapy.selector import Selector
from scrapy import Spider
from wikiSpider.items import Article

class ArticleSpider(Spider):
    name = "article"
    allowed_domains = ["en.wikipedia.org"]
    start_urls = ["http://en.wikipedia.org/wiki/Main_Page",
                  "http://en.wikipedia.org/wiki/Python_%28programming_language%29"]

    def parse(self,response):
        item = Article()
        title = response.xpath('//h1/text()')[0].extract()
        print("Title is:"+title)
        item['title'] = title
        return item

#在wikiSpider 主目录中用如下命令运行ArticleSpider
$ scrapy crawl article
#结果如下：
Title is: Main Page
Title is: Python (programming language)


#定义规则让scrapy可以在每个页面查找URL链接
from scrapy.contrib.spiders import CrawlSpider, Rule
from wikiSpider.items import Article
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor

class ArticleSpider(CrawlSpider):
    name ="article"
    allowed_domains = ["en.wikipedia.org"]
    start_urls = ["http://en.wikipedia.org/wiki/Python_%28programming_language%29"]
    rules = [Rule(SgmlLinkExtractor(allow=('(/wiki)((?!:).)*$'),),
                                            callback="parse_item", follow=Ture)]

    def parse_item(self, response):
        item = Article()
        title = response.xpath('//h1/text()')[0].extract()
        print("Title is:" + title)
        item['title'] = title
        return item
# Ctrl +C 中止程序，否则不会停止

#Scrapy 日志处理：
#Scrapy 生成的调试信息非常有用，但是太啰嗦——可以在Scrapy项目中的setting.py文件中设置日志显示层级：
    LOG_LEVEL = 'ERROR' # 显示 CRITICAL ERROR　两个层级
#Scrapy 日志中有五种层级 CRITICAL ERROR WARNINGT DEBUG INFO ，范围依次递增；
# 日志储存：
    $scrapy crawl article -s LOG_FILE=wiki.log
#scrapy 用Item对象决定要从它浏览的页面中获取哪些信息；支持用不同的输出格式来保存这些信息：CSV JSON XML文件格式
    $ scrapy crawl article -o article.csv -t csv
    $ scrapy crawl article -o article.json - t json
    $ scrapy crawl article -o article,xml -t xml 

    
