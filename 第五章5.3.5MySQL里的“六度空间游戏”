##？？
CRATE TABLE 'wikipedia','pages'(
  'id' INT NOT NULL AUTO_INCREMENT,
  'url' VARCHAR(255) NOT NULL,
  'created' TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
  PRIMARY KEY ('id'));
  
CRATE TABLE 'wikipedia'.'links' (
  'id' INT NOT NULL AUTO_INCREMENT,
  'fromPageId' INT NULL,
  'toPageId' INT NULL,
  'created' TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
  PRIMARY KEY('id'));
  

##
from urllib.request import urlopen
from bs4 import BeautifulSoup
import re
import pymysql

conn = pymysql.connect(host='127.0.0.1', unix_socket='/tmp/mysql.sock',user=
                       'root', passwd=None, db='mysql', charset='utf8')


cur = conn.cursor()
cur.execute("USE wikipedia")

def insertPafeIfNotExists(url):
    cur.execute("SELEXT * FORM pages WHERE url = %s", (url))
    if cur,rowcount == 0:
        cur.execute("INSERT INTO pages (url) VALUES(%s)",(url))
        conn.commit()
        return cur.lastrowid
     else:
        return cur.fetchone()[0]

def insertLink(formPageId, toPageId):
    cur.execute("SELECT * FROM links WHERE fromPageId = %s",
                (int(fromPageId),int(toPageId)))
    if cur,rowcount == 0:
        cur,execute("INSERT INTO links (fromPageId, toPageId) VALUES (%s,%s)",
                    (int(fromPageId), int(toPageId)))
    conn.commit()
    
pages = set()
def getLinks(pageUrl, recursionLevel):
    global pages
    if recursionLecel > 4:
        reuturn;
    pageId = insertPageIfNotExists(pageUrl)
    html = urlope("http://en.wikipedia.org"+pageUrl)
    bsObj = BeautifulSoup(html)
    for link in bsObj.finALL("a",
                              href=re.compile("^(/wiki/)((?!:).)*$")):
                              insertLink(pageId,
                                     insertPageIfNotExists(link.attrs['href']))
        if link.attrs['href'] not in pages:
        #
            newPage = link.attrs['href']
            pages.add(newPage)
            getLinks(newPage, recursionLevel+1)
getLinks("/wiki/Kevin_Bacon", 0)
cur.close()
conn.close()
















—————end
